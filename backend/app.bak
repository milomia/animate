from fastapi import FastAPI, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import re
from llama_index.llms.ollama import Ollama
from llama_parse import LlamaParse
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, PromptTemplate
from llama_index.readers.web import SimpleWebPageReader
from llama_index.core.embeddings import resolve_embed_model
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.core.agent import ReActAgent
from pydantic import BaseModel
from llama_index.core.output_parsers import PydanticOutputParser
from llama_index.core.query_pipeline import QueryPipeline
from dotenv import load_dotenv
import os
import ast
import nest_asyncio
#; nest_asyncio.apply()


def query_llm(prompt: str):
    load_dotenv()

    llm = Ollama(model="mistral", request_timeout=30.0)

    parser = LlamaParse(result_type="markdown")

    file_extractor = {".docx": parser}
    documents = SimpleDirectoryReader("./files", file_extractor=file_extractor).load_data()

    ##documents = SimpleWebPageReader(html_to_text=True).load_data(["https://www.bbc.com/news"])

    embed_model = resolve_embed_model("local:BAAI/bge-m3")
    vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
    query_engine = vector_index.as_query_engine(llm=llm)

    result = query_engine.query(prompt)
    print(result)



app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins
    allow_methods=["*"],  # Allow all methods
    allow_headers=["*"],  # Allow all headers
)

@app.post("/api/upload")
async def upload_file(file: UploadFile = File(...), filename: str = Form(...)):
    breakpoint()
    file_location = f"files/{filename}"
    with open(file_location, "wb") as f:
        f.write(await file.read())
    return JSONResponse(content={"message": f"File '{filename}' uploaded successfully"})

@app.post("/api/query")
async def upload_file(query: str = Form(...)):
    await query_llm(query)
    return
